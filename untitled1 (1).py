# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4gnZu6c6T50zx65lUAVLHnKJn8oZipO
"""

# STEP 1: ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
!apt-get install -y tesseract-ocr tesseract-ocr-ara
!pip install -q pytesseract Pillow requests
# llama-cpp Ù„ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ù…Ø­Ù„ÙŠØ§Ù‹
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.44  --no-cache-dir

# STEP 2: ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ù…Ù† Hugging Face Ù…Ø¨Ø§Ø´Ø±Ø© (Q4_0)
import os

model_url = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf"
model_path = "mistral.Q4_0.gguf"

if not os.path.exists(model_path):
    print("â¬ Downloading Mistral model (Q4_0)...")
    import urllib.request
    urllib.request.urlretrieve(model_url, model_path)
    print("âœ… Model downloaded!")
else:
    print("âœ” Model already exists.")

# STEP 3: Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø©
from google.colab import files
uploaded = files.upload()
image_path = next(iter(uploaded))

# STEP 4: ØªØ´ØºÙŠÙ„ OCR Ø¹Ù„Ù‰ Ø§Ù„ÙØ§ØªÙˆØ±Ø©
from PIL import Image
import pytesseract

img = Image.open(image_path)
text = pytesseract.image_to_string(img, lang='eng+ara')

print("ğŸ“„ Ù†Øµ Ø§Ù„ÙØ§ØªÙˆØ±Ø©:\n")
print(text)

# STEP 5: ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ
from llama_cpp import Llama

llm = Llama(model_path="mistral.Q4_0.gguf", n_ctx=2048)

# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
prompt = f"""
[INST]
Analyze the following invoice.
Extract all expenses with amounts, and classify them into categories like:
food, groceries, transport, entertainment, utilities and other.
Return a JSON object and do not give any comment or explanation.

Invoice:
{text}
[/INST]
"""

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
response = llm(prompt, max_tokens=512, stop=["</s>"])

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
output = response["choices"][0]["text"]
print("\nğŸ“Š Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„Ù…ØµÙ†ÙØ©:\n")
print(output)

"""#### Attempting to connect directly to Firebase
unsuccessful ğŸ˜…
"""

# STEP 1: Install Required Packages
!pip install -q llama-cpp-python==0.2.44 requests

# STEP 2: Load Mistral Model
from llama_cpp import Llama
import requests
import time

# Load the Mistral model (must be downloaded in advance)
llm = Llama(model_path="mistral-7b-instruct-v0.1.Q4_0.gguf", n_ctx=2048)

# STEP 3: Set Firebase Database URL
FIREBASE_URL = "https://YOUR_PROJECT_ID.firebaseio.com"  # <-- Replace with your real Firebase Realtime DB URL

# Optional: If using authentication, add auth token
FIREBASE_AUTH = ""  # leave blank for open DBs

# Helper to build the full URL to an endpoint in Firebase
def firebase_url(path):
    url = f"{FIREBASE_URL}/{path}.json"
    if FIREBASE_AUTH:
        url += f"?auth={FIREBASE_AUTH}"
    return url

# STEP 4: Read the latest input from Firebase

def get_latest_prompt():
    try:
        res = requests.get(firebase_url("prompts"))
        data = res.json()
        if not data:
            return None, None
        # Get the last prompt entry
        sorted_items = sorted(data.items(), key=lambda x: x[1].get("timestamp", 0), reverse=True)
        prompt_id, prompt_data = sorted_items[0]
        return prompt_id, prompt_data["text"]
    except Exception as e:
        print("Error reading Firebase:", e)
        return None, None

# STEP 5: Send prompt to model and get response

def run_model(prompt_text):
    formatted_prompt = f"""
    [INST] {prompt_text} [/INST]
    """
    response = llm(formatted_prompt, max_tokens=512, stop=["</s>"])
    return response["choices"][0]["text"].strip()

# STEP 6: Write the result back to Firebase

def save_response(prompt_id, response_text):
    data = {"response": response_text, "timestamp": int(time.time())}
    res = requests.patch(firebase_url(f"prompts/{prompt_id}"), json=data)
    return res.status_code == 200

# STEP 7: Full process: check Firebase âœ run model âœ update Firebase

prompt_id, prompt_text = get_latest_prompt()
if prompt_text:
    print("âœ… Got prompt:", prompt_text)
    answer = run_model(prompt_text)
    print("ğŸ¤– Model reply:", answer)
    success = save_response(prompt_id, answer)
    if success:
        print("ğŸ“¤ Response saved to Firebase.")
    else:
        print("âŒ Failed to save response.")
else:
    print("ğŸ“­ No prompts found in Firebase.")

