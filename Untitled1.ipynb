{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-ara\n",
        "!pip install -q pytesseract Pillow requests\n",
        "# llama-cpp Ù„ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ù…Ø­Ù„ÙŠØ§Ù‹\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.44  --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8paXp6Urt9u",
        "outputId": "e8e1d72a-5bf6-41c9-d38e-09efc16a6677"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-ara is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting llama-cpp-python==0.2.44\n",
            "  Downloading llama_cpp_python-0.2.44.tar.gz (36.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m296.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.44) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.44) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.44)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.44) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.44) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m251.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.44-cp311-cp311-linux_x86_64.whl size=20877913 sha256=6fb30d88e26ab38bc4c7f2ae0f139b81fafe349342ec7f45d3e4abbf8b0fb00a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cn5r4vaa/wheels/fa/7e/9a/6a4e5377e7df680b778505efb19cbc24d5343c5612589bdce3\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ù…Ù† Hugging Face Ù…Ø¨Ø§Ø´Ø±Ø© (Q4_0)\n",
        "import os\n",
        "\n",
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf\"\n",
        "model_path = \"mistral.Q4_0.gguf\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"â¬ Downloading Mistral model (Q4_0)...\")\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(model_url, model_path)\n",
        "    print(\"âœ… Model downloaded!\")\n",
        "else:\n",
        "    print(\"âœ” Model already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_h5KbeSr0Ua",
        "outputId": "9a834276-9396-4df3-f11c-77ab2adc106a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading Mistral model (Q4_0)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„ÙØ§ØªÙˆØ±Ø©\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "image_path = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "J1RKlIVur1bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: ØªØ´ØºÙŠÙ„ OCR Ø¹Ù„Ù‰ Ø§Ù„ÙØ§ØªÙˆØ±Ø©\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "img = Image.open(image_path)\n",
        "text = pytesseract.image_to_string(img, lang='eng+ara')\n",
        "\n",
        "print(\"ğŸ“„ Ù†Øµ Ø§Ù„ÙØ§ØªÙˆØ±Ø©:\\n\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "VRAuamxpr3C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Mistral Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"mistral.Q4_0.gguf\", n_ctx=2048)\n",
        "\n",
        "# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª\n",
        "prompt = f\"\"\"\n",
        "[INST]\n",
        "Analyze the following invoice.\n",
        "Extract all expenses with amounts, and classify them into categories like:\n",
        "food, groceries, transport, entertainment, utilities and other.\n",
        "Return a JSON object.\n",
        "\n",
        "Invoice:\n",
        "{text}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "response = llm(prompt, max_tokens=512, stop=[\"</s>\"])\n",
        "\n",
        "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "output = response[\"choices\"][0][\"text\"]\n",
        "print(\"\\nğŸ“Š Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„Ù…ØµÙ†ÙØ©:\\n\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "D0yzaR9crGF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attempting to connect directly to Firebase\n",
        "unsuccessful ğŸ˜…"
      ],
      "metadata": {
        "id": "dvlA44JTrPZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Required Packages\n",
        "!pip install -q llama-cpp-python==0.2.44 requests\n",
        "\n",
        "# STEP 2: Load Mistral Model\n",
        "from llama_cpp import Llama\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Load the Mistral model (must be downloaded in advance)\n",
        "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_0.gguf\", n_ctx=2048)\n",
        "\n",
        "# STEP 3: Set Firebase Database URL\n",
        "FIREBASE_URL = \"https://YOUR_PROJECT_ID.firebaseio.com\"  # <-- Replace with your real Firebase Realtime DB URL\n",
        "\n",
        "# Optional: If using authentication, add auth token\n",
        "FIREBASE_AUTH = \"\"  # leave blank for open DBs\n",
        "\n",
        "# Helper to build the full URL to an endpoint in Firebase\n",
        "def firebase_url(path):\n",
        "    url = f\"{FIREBASE_URL}/{path}.json\"\n",
        "    if FIREBASE_AUTH:\n",
        "        url += f\"?auth={FIREBASE_AUTH}\"\n",
        "    return url\n",
        "\n",
        "# STEP 4: Read the latest input from Firebase\n",
        "\n",
        "def get_latest_prompt():\n",
        "    try:\n",
        "        res = requests.get(firebase_url(\"prompts\"))\n",
        "        data = res.json()\n",
        "        if not data:\n",
        "            return None, None\n",
        "        # Get the last prompt entry\n",
        "        sorted_items = sorted(data.items(), key=lambda x: x[1].get(\"timestamp\", 0), reverse=True)\n",
        "        prompt_id, prompt_data = sorted_items[0]\n",
        "        return prompt_id, prompt_data[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(\"Error reading Firebase:\", e)\n",
        "        return None, None\n",
        "\n",
        "# STEP 5: Send prompt to model and get response\n",
        "\n",
        "def run_model(prompt_text):\n",
        "    formatted_prompt = f\"\"\"\n",
        "    [INST] {prompt_text} [/INST]\n",
        "    \"\"\"\n",
        "    response = llm(formatted_prompt, max_tokens=512, stop=[\"</s>\"])\n",
        "    return response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "# STEP 6: Write the result back to Firebase\n",
        "\n",
        "def save_response(prompt_id, response_text):\n",
        "    data = {\"response\": response_text, \"timestamp\": int(time.time())}\n",
        "    res = requests.patch(firebase_url(f\"prompts/{prompt_id}\"), json=data)\n",
        "    return res.status_code == 200\n",
        "\n",
        "# STEP 7: Full process: check Firebase âœ run model âœ update Firebase\n",
        "\n",
        "prompt_id, prompt_text = get_latest_prompt()\n",
        "if prompt_text:\n",
        "    print(\"âœ… Got prompt:\", prompt_text)\n",
        "    answer = run_model(prompt_text)\n",
        "    print(\"ğŸ¤– Model reply:\", answer)\n",
        "    success = save_response(prompt_id, answer)\n",
        "    if success:\n",
        "        print(\"ğŸ“¤ Response saved to Firebase.\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to save response.\")\n",
        "else:\n",
        "    print(\"ğŸ“­ No prompts found in Firebase.\")\n"
      ],
      "metadata": {
        "id": "vxxD6buIDKwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_57tBOACDKmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e9ItkrM9DKbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ckQwuSmJDKQT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTisHZ78xe4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}