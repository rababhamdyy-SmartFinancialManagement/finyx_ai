budgeting model (Business Edition) â€” LLM Fine-Tuning with LoRA and Chat Templates
This project demonstrates how to fine-tune a large language model using Unsloth and LoRA adapters, focusing on financial-style instruction tuning with multiturn conversation data. 
It leverages the FineTome-100k dataset in ShareGPT style and transforms it into HuggingFace's multiturn format.


ğŸ”§ Features
~ Integration with Unsloth for memory-efficient fine-tuning.

~ Adapter-based training using LoRA to reduce training costs.

~ Support for multiple chat templates (LLaMA, Mistral, Zephyr, etc.).

~ Data preprocessing to convert datasets into multi-turn conversational format.

~ Model format compliant with LLaMA 3.1 Instruct and other chat-style LLMs.


ğŸ“ Project Structure
Notebook: financial_project.ipynb â€” contains all code from setup, preprocessing, adapter application, and template formatting.
Dataset: Uses mlabonne/FineTome-100k from Hugging Face.
Chat Templates: Converted into LLaMA 3.1-style conversation format using get_chat_template.



ğŸš€ How to Use:
Install Dependencies
bash
Copy
Edit
pip install unsloth datasets
Run Notebook

Open financial_project.ipynb in Jupyter or Colab and execute cells sequentially.
Modify chat template and adjust get_chat_template(model_type="llama3") depending on the model you're fine-tuning.


ğŸ“Š Results
~ Supports generation of tokenized multiturn conversations.

~ Efficient parameter updates (~10%) using LoRA for scalability.


ğŸ“¦ Dependencies
unsloth
datasets
transformers (likely required downstream)
Compatible with Hugging Face Hub datasets and models


ğŸ“š Dataset
Name: FineTome-100k
Style: ShareGPT / ChatML-style financial conversations


ğŸ§  Model Compatibility
Supports the following models through template configuration:
LLaMA / LLaMA 2 / LLaMA 3.1
Mistral
Zephyr
Alpaca
Vicuna
Phi3
ChatML


ğŸ“ Note:
Both the .py and .ipynb files are the same


ğŸ§‘â€ğŸ’» Author:
Elsayed Zaki 
